AWSTemplateFormatVersion: '2010-09-09'
Description: >
  Custom Lambda function resources to operate on cross-account resources
  and dynamically create pipelines.

Resources:
  StackMakerLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      MemorySize: 128
      Role: !GetAtt StackMakerLambdaRole.Arn
      Runtime: python3.6
      Timeout: 900
      Code:
        ZipFile: |
          import os, boto3, json, signal, traceback
          from botocore.vendored import requests
          from botocore.exceptions import ClientError

          def get_cfn_parameters(e):
              params = []
              cfnp = e['ResourceProperties']['CfnParameters']
              for p in cfnp.keys():
                  params.append({"ParameterKey": p, "ParameterValue": cfnp[p]})
              return params

          def get_client(service, e, c):
              sts_client = boto3.client('sts')
              response = sts_client.assume_role(RoleArn=e['ResourceProperties']['RoleArn'],RoleSessionName='CfnStack')
              session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken'])
              return session.client(service)

          def update(e, c):
              cf_client = get_client("cloudformation", e, c)
              rp = e['ResourceProperties']
              try:
                response = cf_client.update_stack(
                    StackName=e["PhysicalResourceId"],
                    TemplateBody=rp['TemplateBody'],
                    Parameters=get_cfn_parameters(e),
                    Capabilities=rp['Capabilities']
                )
              except ClientError as er:
                if "No updates are to be performed" not in str(er):
                    raise
                return e["PhysicalResourceId"]
              waiter = cf_client.get_waiter('stack_update_complete')
              waiter.wait(StackName=e["PhysicalResourceId"])
              return response['StackId']

          def create(e, c):
              cf_client = get_client("cloudformation", e, c)
              rp = e['ResourceProperties']
              response = cf_client.create_stack(
                StackName=rp['StackName'],
                TemplateBody=rp['TemplateBody'],
                Parameters=get_cfn_parameters(e),
                Capabilities=rp['Capabilities']
              )
              waiter = cf_client.get_waiter('stack_create_complete')
              waiter.wait(StackName=rp['StackName'])
              return response['StackId']

          def delete(e, c):
              cf_client = get_client("cloudformation", e, c)
              response = cf_client.delete_stack(StackName=e["PhysicalResourceId"])
              waiter = cf_client.get_waiter('stack_delete_complete')
              waiter.wait(StackName=e["PhysicalResourceId"])
              return True

          def lambda_handler(e, c):
              signal.alarm(int(c.get_remaining_time_in_millis() / 1000) - 1)
              try:
                  print('event: {}'.format(json.dumps(e)))
                  if e['RequestType'] == 'Create':
                      e['PhysicalResourceId'] = create(e, c)
                      send_response(e, c, "SUCCESS", {"Message": "Resource creation successful!"})
                  elif e['RequestType'] == 'Update':
                      e['PhysicalResourceId'] = update(e, c)
                      send_response(e, c, "SUCCESS", {"Message": "Resource update successful!"})
                  elif e['RequestType'] == 'Delete':
                      delete(e, c)
                      send_response(e, c, "SUCCESS", {"Message": "Resource deletion successful!"})
                  else:
                      send_response(e, c, "FAILED", {"Message": "Unexpected event received from CF"})
              except Exception as er:
                  traceback.print_exc()
                  send_response(e, c, "FAILED", {"Message": "Exception during processing"})

          def send_response(e, c, response_status, response_data):
              if not e.get('PhysicalResourceId'):
                  e['PhysicalResourceId'] = c.log_stream_name
              _rb = {
                  "Status": response_status,
                  "Reason": "See the details in CloudWatch Log Stream: " + c.log_stream_name,
                  "StackId": e['StackId'],
                  "RequestId": e['RequestId'],
                  "LogicalResourceId": e['LogicalResourceId'],
                  "PhysicalResourceId": e['PhysicalResourceId'],
                  "Data": response_data
              }
              rb = json.dumps(_rb)

              print('ResponseBody: %s', rb)
              headers = {
                  'content-type': '',
                  'content-length': str(len(rb))
              }
              try:
                  response = requests.put(e['ResponseURL'], data=rb, headers=headers)
                  print("CF returned status code: " + response.reason)
              except Exception as e:
                  print("send(..) failed executing requests.put(..): " + str(e))
                  raise

          def timeout_handler(_signal, _frame):
              raise Exception('Time exceeded')

          signal.signal(signal.SIGALRM, timeout_handler)
  StackMakerLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: CfnStackAssumeRole
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource:
            - arn:aws:logs:*:*:*
          - Effect: Allow
            Action:
            - sts:AssumeRole
            Resource: "*"
          - Effect: Allow
            Action:
            - lambda:AddPermission
            - lambda:RemovePermission
            Resource:  "*"
          - Effect: Allow
            Action:
            - events:PutRule
            - events:DeleteRule
            - events:PutTargets
            - events:RemoveTargets
            Resource:  "*"
  DynamicPipelineCleanupLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      MemorySize: 128
      Runtime: python3.6
      Timeout: 900
      Role: !GetAtt DynamicPipelineCleanupLambdaRole.Arn
      Environment:
        Variables:
          LOGGING_LEVEL: INFO
          AppName: !Ref AppName
      Code:
        ZipFile: |
          import os, boto3, logging, json, signal, traceback
          from botocore.vendored import requests

          logger = logging.getLogger()
          logging.basicConfig()
          LOGGING_LEVEL = os.environ['LOGGING_LEVEL']
          logger.setLevel(LOGGING_LEVEL)

          def delete_stack(event, context):
              sts_client = boto3.client('sts')
              response = sts_client.assume_role(
                  RoleArn=event['ResourceProperties']['RoleArn'],
                  RoleSessionName='CleanupChildStacks'
              )
              session = boto3.Session(
                  aws_access_key_id=response['Credentials']['AccessKeyId'],
                  aws_secret_access_key=response['Credentials']['SecretAccessKey'],
                  aws_session_token=response['Credentials']['SessionToken']
              )
              cf_client = session.client('cloudformation')
              StackName = event['ResourceProperties']['StackName']
              response = cf_client.delete_stack(
                  StackName=StackName
              )
              waiter = cf_client.get_waiter('stack_delete_complete')
              waiter.wait(StackName=StackName)
              logger.info('successfully deleted CloudFormation stack:{}'.format(StackName))
              return True

          def lambda_handler(event, context):
              '''Handle Lambda event from AWS'''
              # Setup alarm for remaining runtime minus a second
              signal.alarm(int(context.get_remaining_time_in_millis() / 1000) - 1)
              try:
                  logger.info('event: {}'.format(json.dumps(event)))
                  if event['RequestType'] == 'Create':
                      logger.info('CREATE!')
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource creation successful!"})
                  elif event['RequestType'] == 'Update':
                      logger.info('UPDATE!')
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource update successful!"})
                  elif event['RequestType'] == 'Delete':
                      logger.info('DELETE!')
                      delete_stack(event, context)
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource deletion successful!"})
                  else:
                      logger.info('FAILED!')
                      send_response(event, context, "FAILED",
                                    {"Message": "Unexpected event received from CloudFormation"})
              except: #pylint: disable=W0702
                  logger.info('FAILED!')
                  traceback.print_exc()
                  send_response(event, context, "FAILED", {
                      "Message": "Exception during processing"})


          def send_response(event, context, response_status, response_data):
              response_body = json.dumps({
                  "Status": response_status,
                  "Reason": "See the details in CloudWatch Log Stream: " + context.log_stream_name,
                  "PhysicalResourceId": context.log_stream_name,
                  "StackId": event['StackId'],
                  "RequestId": event['RequestId'],
                  "LogicalResourceId": event['LogicalResourceId'],
                  "Data": response_data
              })

              headers = {
                  'content-type': '',
                  'content-length': str(len(response_body))
              }
              response = requests.put(event['ResponseURL'],
                                      data=response_body,
                                      headers=headers)
              logger.info("CloudFormation returned status code: " + response.reason)

          def timeout_handler(_signal, _frame):
              raise Exception('Time exceeded')


          signal.signal(signal.SIGALRM, timeout_handler)
  DynamicPipelineCleanupLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: DynamicPipelineCleanup
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource:
            - arn:aws:logs:*:*:*
          - Effect: Allow
            Action:
              - sts:AssumeRole
            Resource:
              - !Sub arn:aws:iam::${DevAwsAccountId}:role/CodePipelineServiceRole-${AWS::Region}-${DevAwsAccountId}-dev
              - !Sub arn:aws:iam::${ProdAwsAccountId}:role/CodePipelineServiceRole-${AWS::Region}-${ProdAwsAccountId}-prod
  DynamicPipelineMakerLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      MemorySize: 128
      Runtime: python3.6
      Timeout: 900
      Role: !GetAtt DynamicPipelineMakerLambdaRole.Arn
      Environment:
        Variables:
          LOGGING_LEVEL: INFO
          TemplateURL: !Sub 
            - https://${S3Bucket}.s3.${S3Region}.${AWS::URLSuffix}/${S3KeyPrefix}templates/pipeline.template.yaml
            - S3Region: !If [UsingDefaultBucket, !Ref 'AWS::Region', !Ref S3BucketRegion]
              S3Bucket: !If [UsingDefaultBucket, !Sub '${S3BucketName}-${AWS::Region}', !Ref S3BucketName]
          StackName: !Ref ParentStackName
          DynamicPipelineCleanupLambdaArn: !GetAtt DynamicPipelineCleanupLambda.Arn
      Code:
        ZipFile: |
          import os, boto3, logging, json, re, concurrent.futures

          logger = logging.getLogger()
          logging.basicConfig()
          LOGGING_LEVEL = os.environ['LOGGING_LEVEL']
          logger.setLevel(LOGGING_LEVEL)

          # override boto3 logging configuration
          logging.getLogger('boto3').setLevel(logging.ERROR)
          logging.getLogger('boto3').propagate = False
          logging.getLogger('botocore').setLevel(logging.ERROR)
          logging.getLogger('botocore').propagate = False

          max_workers = 5

          cf_client = boto3.client('cloudformation')

          def sanitize_branch_name(branch):
              suffix = re.sub('[^0-9a-zA-Z]+', '-', branch)
              StackName = '{}-PPS-branch-{}'.format(os.environ['StackName'], suffix)[:127]
              return suffix, StackName

          def create_pipeline_stack(branch, customParameters):
              suffix, StackName = sanitize_branch_name(branch)
              logger.info('creating stack: {}'.format(StackName))
              response = cf_client.create_stack(
                  StackName=StackName,
                  TemplateURL=os.environ['TemplateURL'],
                  Parameters=[
                      {
                          'ParameterKey': 'Branch',
                          'ParameterValue': branch
                      },
                      {
                          'ParameterKey': 'Suffix',
                          'ParameterValue': suffix
                      },
                      {
                          'ParameterKey': 'DynamicPipelineCleanupLambdaArn',
                          'ParameterValue': os.environ['DynamicPipelineCleanupLambdaArn']
                      },
                  ]+list(map(lambda x: {"ParameterKey": x, "ParameterValue": customParameters[x]}, customParameters))
              )
              waiter = cf_client.get_waiter('stack_create_complete')
              waiter.wait(StackName=StackName)
              logger.info('successfully created new CloudFormation stack:{}'.format(StackName))
              return True

          def delete_pipeline_stack(branch):
              suffix, StackName = sanitize_branch_name(branch)
              response = cf_client.delete_stack(
                  StackName=StackName
              )
              waiter = cf_client.get_waiter('stack_delete_complete')
              waiter.wait(StackName=StackName)
              logger.info('successfully deleted CloudFormation stack:{}'.format(StackName))
              return True

          def lambda_handler(event, context):
              logger.info('event:{}'.format(json.dumps(event)))
              result_futures = []
              with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                  for record in event['Records']:
                      for reference in record['codecommit']['references']:
                          logger.info('reference:{}'.format(json.dumps(reference)))
                          if reference['ref'].split('/')[1] == 'heads':
                              # it is branch
                              branch = reference['ref'].split('/',2)[-1]
                              if branch == 'master':
                                  logger.info('skipping branch master, pipeline for this branch created by main stack')
                                  continue
                              if reference.get('created'):
                                  logger.info('created new branch:{}, creating pipeline for this branch'.format(branch))
                                  future = executor.submit(create_pipeline_stack, branch, json.loads(record['customData']))
                                  result_futures.append(future)
                              elif reference.get('deleted'):
                                  logger.info('deleted old branch:{}, deleting pipeline for this branch'.format(branch))
                                  future = executor.submit(delete_pipeline_stack, branch)
                                  result_futures.append(future)

              for future in result_futures:
                  future.result()

              return